{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60ec6048-44e4-4118-b16a-9c4c9cc78a3b",
   "metadata": {},
   "source": [
    "# How to deal with complex/large Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9281ac79-47cd-49d4-bdd4-7f5c173a947d",
   "metadata": {},
   "source": [
    "In the previous notebook, we developed a solution for various types of files and data formats commonly found in organizations, and this covers 90% of the use cases. However, you will find that there are issues when dealing with questions that require answers from complex files. The complexity of these files arises from their length and the way information is distributed within them. Large documents are always a challenge for Search Engines.\n",
    "\n",
    "One example of such complex files is Technical Specification Guides or Product Manuals, which can span hundreds of pages and contain information in the form of images, tables, forms, and more. Books are also complex due to their length and the presence of images or tables.\n",
    "\n",
    "These files are typically in PDF format. To better handle these PDFs, we need a smarter parsing method that treats each document as a special source and processes them page by page. The objective is to obtain more accurate and faster answers from our system. Fortunately, there are usually not many of these types of documents in an organization, allowing us to make exceptions and treat them differently.\n",
    "\n",
    "If your use case is just PDFs, for example, you can just use [PyPDF library](https://pypi.org/project/pypdf/) or [Azure AI Document Intelligence SDK (former Form Recognizer)](https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/overview?view=doc-intel-3.0.0), vectorize using OpenAI API and push the content to a vector-based index. And this is problably the simplest and fastest way to go.  However if your use case entails connecting to a datalake, or Sharepoint libraries or any other document data source with thousands of documents with multiple file types and that can change dynamically, then you would want to use the Ingestion and Document Cracking and AI-Enrichment capabilities of Azure Search engine, Notebooks 1-3, and avoid a lot of painful custom code. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15f6044e-463f-4988-bc46-a3c3d641c15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "import langchain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma, FAISS\n",
    "from langchain import OpenAI, VectorDBQA\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "\n",
    "from common.prompts_povel import COMBINE_QUESTION_PROMPT, COMBINE_PROMPT, COMBINE_PROMPT_TEMPLATE\n",
    "from common.utils_povel import (\n",
    "    parse_pdf,\n",
    "    parse_pdf_from_blob,\n",
    "    read_pdf_files,\n",
    "    text_to_base64,\n",
    "    DocSearchTool,\n",
    "    get_search_results,\n",
    "    model_tokens_limit,\n",
    "    num_tokens_from_docs,\n",
    "    num_tokens_from_string)\n",
    "\n",
    "\n",
    "from IPython.display import Markdown, HTML, display  \n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "    \n",
    "os.makedirs(\"data/books/\",exist_ok=True)\n",
    "    \n",
    "\n",
    "# BLOB_CONTAINER_NAME = \"digge-ekonomi-dokument\"\n",
    "# BASE_CONTAINER_URL = \"https://blobstoragejd5ypzfx2l6vi.blob.core.windows.net/digge-ekonomi-dokument/\"\n",
    "\n",
    "# BLOB_CONTAINER_NAME = \"digge-100-dokument\"\n",
    "# BASE_CONTAINER_URL = \"https://blobstoragejd5ypzfx2l6vi.blob.core.windows.net/digge-100-dokument/\"\n",
    "\n",
    "BLOB_CONTAINER_NAME=\"test2\"\n",
    "BASE_CONTAINER_URL=\"https://blobstoragejd5ypzfx2l6vi.blob.core.windows.net/test2/\"\n",
    "\n",
    "MODEL = \"gpt-35-turbo-16k\" # options: gpt-35-turbo, gpt-35-turbo-16k, gpt-4, gpt-4-32k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "331692ba-b68e-4b99-9bae-5057da9a389d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the ENV variables that Langchain needs to connect to Azure OpenAI\n",
    "os.environ[\"OPENAI_API_BASE\"] = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.environ[\"AZURE_OPENAI_API_KEY\"]\n",
    "os.environ[\"OPENAI_API_VERSION\"] = os.environ[\"AZURE_OPENAI_API_VERSION\"]\n",
    "os.environ[\"OPENAI_API_TYPE\"] = \"azure\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "594ff0d4-56e3-4bed-843d-28c7a092069b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = OpenAIEmbeddings(deployment=\"text-embedding-ada-002\", chunk_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788cc0db-9dae-45f2-8943-2b6fa32fcc75",
   "metadata": {},
   "source": [
    "### What to use: pyPDF or AI Documment Intelligence API (Form Recognizer)?\n",
    "\n",
    "In `utils.py` there is a **parse_pdf()** function. This utility function can parse local files using PyPDF library and can also parse local or from_url PDFs files using Azure AI Document Intelligence (Former Form Recognizer).\n",
    "\n",
    "If `form_recognizer=False`, the function will parse the PDF using the python pyPDF library, which 75% of the time does a good job.<br>\n",
    "\n",
    "Setting `form_recognizer=True`, is the best (and slower) parsing method using AI Documment Intelligence API (former known as Form Recognizer). You can specify the prebuilt model to use, the default is `model=\"prebuilt-document\"`. However, if you have a complex document with tables, charts and figures , you can try\n",
    "`model=\"prebuilt-layout\"`, and it will capture all of the nuances of each page (it takes longer of course).\n",
    "\n",
    "**Note: Many PDFs are scanned images. For example, any signed contract that was scanned and saved as PDF will NOT be parsed by pyPDF. Only AI Documment Intelligence API will work.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As demonstrated above, Azure Document Intelligence proves to be superior to pyPDF. **For production scenarios, we strongly recommend using Azure Document Intelligence consistently**. When doing so, it's important to make a wise choice between the available models, such as \"prebuilt-document,\" \"prebuilt-layout,\" or others. You can find more information on model selection [HERE](https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/choose-model-feature?view=doc-intel-3.0.0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Text from Kontoplan.pdf ...\n",
      "Extracting text using Azure Document Intelligence\n",
      "Parsing took: 14.954447 seconds\n",
      "Kontoplan.pdf contained 95 pages\n",
      "\n",
      "Extracting Text from Kundnummer i Agresso.pdf ...\n",
      "Extracting text using Azure Document Intelligence\n",
      "Parsing took: 5.259512 seconds\n",
      "Kundnummer i Agresso.pdf contained 1 pages\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "# # Create a blob service client\n",
    "# blob_service_client = BlobServiceClient.from_connection_string(os.environ[\"BLOB_CONNECTION_STRING\"])\n",
    "\n",
    "# # Get a reference to the container\n",
    "# container_client = blob_service_client.get_container_client(BLOB_CONTAINER_NAME)\n",
    "\n",
    "# # List all blobs in the container\n",
    "# blob_list = container_client.list_blobs()\n",
    "\n",
    "# model=\"prebuilt-layout\"\n",
    "# book_pages_map = dict()\n",
    "\n",
    "# # Print all blob names\n",
    "# for document in blob_list:\n",
    "#     if document.name.endswith(\".pdf\"):\n",
    "#         print(\"Extracting Text from\",document.name,\"...\")\n",
    "\n",
    "#         # Capture the start time\n",
    "#         start_time = time.time()\n",
    "\n",
    "#         # Get a reference to the blob\n",
    "#         blob_client = container_client.get_blob_client(document.name)\n",
    "#         # print(document.name)\n",
    "        \n",
    "#         # Download the blob to a stream\n",
    "#         stream = blob_client.download_blob().readall()\n",
    "\n",
    "#         # with open(stream, \"rb\") as filename:\n",
    "#         book_map = parse_pdf_from_blob(file=stream, form_recognizer=True, verbose=True)\n",
    "#         book_pages_map[document.name]= book_map\n",
    "\n",
    "#         # Capture the end time and Calculate the elapsed time\n",
    "#         end_time = time.time()\n",
    "#         elapsed_time = end_time - start_time\n",
    "\n",
    "#         print(f\"Parsing took: {elapsed_time:.6f} seconds\")\n",
    "#         print(f\"{document.name} contained {len(book_map)} pages\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5f9b7d-99e6-426d-a47e-343c7e8b492e",
   "metadata": {},
   "source": [
    "## Create Vector-based index\n",
    "\n",
    "\n",
    "Now that we have the content of the book's chunks (each page of each book) in the dictionary `book_pages_map`, let's create the Vector-based index in our Azure Search Engine where this content is going to land"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d46e7c5-49c4-40f3-bb2d-79a9afeab4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # index_name = \"1b-digge-ekonomi-full\"\n",
    "# # index_name = \"1b-digge-100-full\"\n",
    "# index_name=\"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b07e84b-d306-4bc9-9124-e64f252dd7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Setup the Payloads header\n",
    "# headers = {'Content-Type': 'application/json','api-key': os.environ['AZURE_SEARCH_KEY']}\n",
    "# params = {'api-version': \"2023-07-01-Preview\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2df4db6b-969b-4b91-963f-9334e17a4e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# index_payload = {\n",
    "#     \"name\": index_name,\n",
    "#     \"fields\": [\n",
    "#         {\"name\": \"id\", \"type\": \"Edm.String\", \"key\": \"true\", \"filterable\": \"true\" },\n",
    "#         {\"name\": \"title\",\"type\": \"Edm.String\",\"searchable\": \"true\",\"retrievable\": \"true\"},\n",
    "#         {\"name\": \"content\",\"type\": \"Edm.String\",\"searchable\": \"true\",\"retrievable\": \"true\"},\n",
    "#         {\"name\": \"filepath\", \"type\": \"Edm.String\", \"searchable\": \"true\", \"retrievable\": \"true\", \"sortable\": \"false\", \"filterable\": \"false\", \"facetable\": \"false\"},\n",
    "#         {\"name\": \"url\", \"type\": \"Edm.String\", \"searchable\": \"false\", \"retrievable\": \"true\", \"sortable\": \"false\", \"filterable\": \"false\", \"facetable\": \"false\"},\n",
    "#         {\"name\": \"page_num\",\"type\": \"Edm.Int32\",\"searchable\": \"false\",\"retrievable\": \"true\"},\n",
    "#         {\"name\": \"contentVector\", \"type\": \"Collection(Edm.Single)\", \"searchable\": \"true\", \"retrievable\": \"true\", \"dimensions\": 1536, \"vectorSearchConfiguration\": \"vectorConfig\"}\n",
    "        \n",
    "#     ],\n",
    "#     \"vectorSearch\": {\n",
    "#         \"algorithmConfigurations\": [\n",
    "#             {\n",
    "#                 \"name\": \"vectorConfig\",\n",
    "#                 \"kind\": \"hnsw\"\n",
    "#             }\n",
    "#         ]\n",
    "#     },\n",
    "#     \"semantic\": {\n",
    "#         \"configurations\": [\n",
    "#             {\n",
    "#                 \"name\": \"default\",\n",
    "#                 \"prioritizedFields\": {\n",
    "#                     \"titleField\": {\n",
    "#                         \"fieldName\": \"title\"\n",
    "#                     },\n",
    "#                     \"prioritizedContentFields\": [\n",
    "#                         {\n",
    "#                             \"fieldName\": \"content\"\n",
    "#                         }\n",
    "#                     ],\n",
    "#                     \"prioritizedKeywordsFields\": []\n",
    "#                 }\n",
    "#             }\n",
    "#         ]\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# r = requests.put(os.environ['AZURE_SEARCH_ENDPOINT'] + \"/indexes/\" + index_name,\n",
    "#                  data=json.dumps(index_payload), headers=headers, params=params)\n",
    "# print(r.status_code)\n",
    "# print(r.ok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36691ff0-c4c8-49d0-bfa8-3e076ece0ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to debug errors\n",
    "# r.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc7dda9-4725-410e-9465-54f0298fc758",
   "metadata": {},
   "source": [
    "## Upload the Document chunks and its vectors to the Vector-Based Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73e7600-7902-48d4-b199-9d9dc0a17aa0",
   "metadata": {},
   "source": [
    "The following code will iterate over each chunk of each book and use the Azure Search Rest API upload method to insert each document with its corresponding vector (using OpenAI embedding model) to the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f5c8aa55-1b60-4057-93db-0d4a89993a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading chunks from Kontoplan.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95/95 [00:14<00:00,  6.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading chunks from Kundnummer i Agresso.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  6.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.37 s, sys: 93.1 ms, total: 4.46 s\n",
      "Wall time: 14.6 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# for bookname,bookmap in book_pages_map.items():\n",
    "#     print(\"Uploading chunks from\",bookname)\n",
    "#     for page in tqdm(bookmap):\n",
    "#         try:\n",
    "#             page_num = page[0] + 1\n",
    "#             content = page[2]\n",
    "#             book_url = BASE_CONTAINER_URL + bookname\n",
    "#             upload_payload = {\n",
    "#                 \"value\": [\n",
    "#                     {\n",
    "#                         \"id\": text_to_base64(bookname + str(page_num)),\n",
    "#                         \"title\": f\"{bookname}_page_{str(page_num)}\",\n",
    "#                         \"content\": content,\n",
    "#                         \"contentVector\": embedder.embed_query(content if content!=\"\" else \"-------\"),\n",
    "#                         \"filepath\": bookname,\n",
    "#                         \"url\": book_url,\n",
    "#                         \"page_num\": page_num,\n",
    "#                         \"@search.action\": \"upload\"\n",
    "#                     },\n",
    "#                 ]\n",
    "#             }\n",
    "\n",
    "#             r = requests.post(os.environ['AZURE_SEARCH_ENDPOINT'] + \"/indexes/\" + index_name + \"/docs/index\",\n",
    "#                                  data=json.dumps(upload_payload), headers=headers, params=params)\n",
    "#             if r.status_code != 200:\n",
    "#                 print(r.status_code)\n",
    "#                 print(r.text)\n",
    "#         except Exception as e:\n",
    "#             print(\"Exception:\",e)\n",
    "#             print(content)\n",
    "#             continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715cddcf-af7b-4006-a047-853fc7a66be3",
   "metadata": {},
   "source": [
    "## Query the Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b408798-5527-44ca-9dba-cad2ee726aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION = \"Vilket konto ska jag fakturera egenavgift för hjälpmedel?\"\n",
    "# QUESTION = \"Hur ser rättigheterna ut för enskilda personer gällander GDPR?\"\n",
    "# QUESTION = \"Hur använder jag Agresso?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from common.callbacks import StdOutCallbackHandler\n",
    "\n",
    "cb_handler = StdOutCallbackHandler()\n",
    "cb_manager = CallbackManager(handlers=[cb_handler])\n",
    "\n",
    "llm = AzureChatOpenAI(deployment_name=MODEL, temperature=0.2, max_tokens=800)\n",
    "vector_only_indexes = [\"1b-digge-100-full\", \"1b-digge-ekonomi-full\"]\n",
    "\n",
    "doc_search = DocSearchTool(llm=llm, vector_only_indexes = vector_only_indexes,\n",
    "                           k=10, similarity_k=5, reranker_th=1,\n",
    "                           sas_token=os.environ['BLOB_SAS_TOKEN'],\n",
    "                           callback_manager=cb_manager, return_direct=True,\n",
    "                           # This is how you can edit the default values of name and description\n",
    "                           name=\"@docsearch\",\n",
    "                           description=\"useful when the questions includes the term: @docsearch.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bad pipe message: %s [b\"\\x82\\xd3\\x87\\x10D\\xe1V\\xc3\\x83g|~kx\\xa0x\\x1bp \\xa4'\\x99\\xeaa-d\\x1e\\xf3\\x13R\\x0ew<\\xde\\x11\\x1b\\xc09\\xab\\xdf\\x80>`\\xdck\\x0fc(D\\x05D\\x00\\x08\\x13\\x02\\x13\\x03\\x13\\x01\\x00\\xff\\x01\\x00\\x00\\x8f\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\"]\n",
      "Bad pipe message: %s [b'\\xd4S\\\\\\x1a\\xd1^s\\xde\\xeab\\xc6j\\xcd\\xf5u\\xa0\\xb8, \\xab\\xce\\xa9\\xd3J\\xe4\\xach\\xc0\\x83Zx\\xba\\xf1p\\xad\\xec\\x0e\\x81.\\x8d\\xd6\\xd8R\\x972\\xfft\\xcf\\x91k\\x96\\x00\\x08\\x13\\x02\\x13\\x03\\x13\\x01\\x00\\xff\\x01\\x00\\x00\\x8f\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x00\\x1e\\x00\\x1c\\x04\\x03\\x05\\x03', b'\\x08\\x07\\x08\\x08\\x08', b'\\n\\x08\\x0b\\x08\\x04\\x08\\x05\\x08']\n",
      "Bad pipe message: %s [b'\\x01\\x05\\x01\\x06\\x01']\n",
      "Bad pipe message: %s [b'\\x17_\\xecD=d\\xfd\\x8a\\x14\\x05\\xc9\\x85\\xa7\\xe8\\x8f\\x1d\\xdd\\x9b\\x00\\x00', b\",\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0#\\xc0'\\x00g\\x00@\\xc0\\n\\xc0\\x14\\x009\\x008\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00<\\x005\\x00/\\x00\\x9a\\x00\\x99\\xc0\\x07\\xc0\\x11\\x00\\x96\\x00\\x05\\x00\\xff\"]\n",
      "Bad pipe message: %s [b'']\n",
      "Bad pipe message: %s [b' \\x0b\\xc7Y\\xd0:VS\\xf80\\x95\\xae\\xfd\\xb9\\x13\\x84\\xc3\\x99\\x00\\x00\\xa6\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]', b\"\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0s\\xc0w\\x00\\xc4\\x00\\xc3\\xc0#\\xc0'\\x00g\\x00@\\xc0r\\xc0v\\x00\\xbe\\x00\\xbd\\xc0\\n\\xc0\\x14\\x009\\x008\\x00\\x88\\x00\\x87\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9a\\x00\\x99\\x00E\\x00D\\xc0\\x07\\xc0\\x11\\xc0\\x08\\xc0\\x12\\x00\\x16\\x00\\x13\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00\\xc0\\x00<\\x00\\xba\\x005\\x00\\x84\\x00/\\x00\\x96\\x00A\\x00\\x05\\x00\\n\\x00\\xff\\x01\\x00\\x00j\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\", b'\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x000\\x00.\\x04\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08']\n",
      "Bad pipe message: %s [b'\\x08\\n\\x08\\x0b\\x08\\x04\\x08', b'\\x06\\x04\\x01\\x05']\n",
      "Bad pipe message: %s [b'']\n",
      "Bad pipe message: %s [b'', b'\\x03\\x03']\n",
      "Bad pipe message: %s [b'']\n",
      "Bad pipe message: %s [b'', b'\\x02']\n",
      "Bad pipe message: %s [b'\\xf0\\xb6\\xa0\\x14n0P\\x90\\x89\\x89*\\xe0\\xb2[\\xe3\\xb9\\x02\\xef\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\xc0\\x0f\\xc0\\x05\\x005\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b\\x00\\x18\\x00\\x1a']\n",
      "Bad pipe message: %s [b'\\x05\\x02\\x06']\n",
      "Bad pipe message: %s [b'\\xf2h\\x9c\\xea\\xd1\\x9e;\\r\\t\\x86F.\\x13Szy=\\xdc\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06\\x00\\x17\\x00\\x03\\xc0\\x10\\xc0\\x06\\xc0\\x15\\xc0\\x0b\\xc0\\x01\\x00\\x02\\x00\\x01\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00']\n",
      "Bad pipe message: %s [b'\\x98\\xb5\\n\\x1d']\n",
      "Bad pipe message: %s [b'\\xfbN!\\n20\\xc1\\xb25\\xc7T2O\\x03\\x86\\xf36\\xe2\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06\\x00\\x17\\x00\\x03\\xc0\\x10\\xc0\\x06\\xc0\\x15\\xc0\\x0b\\xc0\\x01\\x00\\x02\\x00\\x01\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00']\n",
      "Bad pipe message: %s [b'\\x00\\t127.0.0.1']\n",
      "Bad pipe message: %s [b'\\x1c&\\xc2\\xbdICt\\xd1i^r\\x9b\\x08\\xba`\\xf4`i\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x00']\n",
      "Bad pipe message: %s [b'\\x0f\\xc0\\x05\\x005\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x15']\n",
      "Bad pipe message: %s [b'\\x00\\x02']\n",
      "Bad pipe message: %s [b'\\xb9\\x81\\x08`\\x07%\\x98\\xdfw\\xaa\\xb7\\x7fz\\x82\\\\B\\x9b\\xfa\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x00', b'7\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18']\n",
      "Bad pipe message: %s [b'\\xe5\\x89\\x06\\xfcV\\xabWG\\x8e\\x87\\xa0\\x82\\x8ca\\x18\\xf7\\x08\\x06\\x00\\x00\\x86\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k']\n",
      "Bad pipe message: %s [b\"\\xbe\\xe0\\xb1@\\xe7\\rj)\\xf2\\xb3m\\xce\\x07\\xba\\xc7&g\\xca\\x00\\x00\\xf4\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00\\xa7\\x00m\\x00:\\x00\\x89\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\x00\\x84\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x00\\xa6\\x00l\\x004\\x00\\x9b\\x00F\\xc01\\xc0-\\xc0)\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\"]\n"
     ]
    }
   ],
   "source": [
    "printmd(doc_search.run(QUESTION))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 - SDK v2",
   "language": "python",
   "name": "python310-sdkv2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
